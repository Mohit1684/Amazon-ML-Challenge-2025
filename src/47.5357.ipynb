{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13368144,"sourceType":"datasetVersion","datasetId":8480392}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nENHANCED PRICE PREDICTION V2\nTarget: <45% SMAPE\nKey improvements: Better embeddings usage, price bucketing, advanced ensembling\n\"\"\"\n\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport re\nimport warnings\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom sklearn.cluster import KMeans\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.linear_model import Ridge\nfrom scipy.stats import skew, kurtosis\n\nwarnings.filterwarnings('ignore')\n\nprint(\"=\" * 80)\nprint(\"ENHANCED PRICE PREDICTION V2 - TARGET <45% SMAPE\")\nprint(\"=\" * 80)\n\n# ==================== CONFIGURATION ====================\nINPUT_PATH = '/kaggle/input/combined2/Combined'\nOUTPUT_PATH = '/kaggle/working/'\n\nTRAIN_CSV = os.path.join(INPUT_PATH, 'train.csv')\nTEST_CSV = os.path.join(INPUT_PATH, 'test.csv')\nTEXT_EMB_PKL = os.path.join(INPUT_PATH, 'Temp/text_embeddings.pkl')\nTRAIN_IMG_EMB_PKL = os.path.join(INPUT_PATH, 'embeddings/train_image_embeddings.pkl')\nTEST_IMG_EMB_PKL = os.path.join(INPUT_PATH, 'embeddings/test_image_embeddings.pkl')\n\n# ==================== UTILITY FUNCTIONS ====================\n\ndef smape(y_true, y_pred):\n    \"\"\"Calculate SMAPE\"\"\"\n    numerator = np.abs(y_pred - y_true)\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n    return np.mean(numerator / denominator)\n\ndef clean_text(text):\n    \"\"\"Clean text\"\"\"\n    if pd.isna(text):\n        return \"\"\n    text = str(text).lower()\n    text = re.sub(r'http\\S+|www\\S+', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef extract_comprehensive_features(text):\n    \"\"\"Extract comprehensive features from text\"\"\"\n    text_lower = text.lower()\n    \n    features = {}\n    \n    # ========== QUANTITY PATTERNS ==========\n    pack_patterns = [\n        (r'(\\d+)\\s*(?:pack|count|ct|piece|pcs|pc)\\b', 1),\n        (r'pack\\s+of\\s+(\\d+)', 1),\n        (r'(\\d+)\\s*-\\s*pack', 1),\n        (r'case\\s+of\\s+(\\d+)', 1),\n        (r'[x×]\\s*(\\d+)', 1),\n        (r'set\\s+of\\s+(\\d+)', 1),\n    ]\n    \n    pack_qty = 1.0\n    for pattern, _ in pack_patterns:\n        match = re.search(pattern, text_lower)\n        if match:\n            pack_qty = float(match.group(1))\n            break\n    \n    features['pack_quantity'] = pack_qty\n    features['log_pack_qty'] = np.log1p(pack_qty)\n    \n    # ========== VOLUME/WEIGHT PATTERNS ==========\n    volume_patterns = [\n        (r'(\\d+(?:\\.\\d+)?)\\s*(ml|l|liter|litre|milliliter)', 1),\n        (r'(\\d+(?:\\.\\d+)?)\\s*(fl\\s*oz|fluid\\s*ounce)', 29.5735),\n        (r'(\\d+(?:\\.\\d+)?)\\s*(oz|ounce)(?!\\s*fl)', 28.3495),\n        (r'(\\d+(?:\\.\\d+)?)\\s*(g|gram)(?!allon)', 1),\n        (r'(\\d+(?:\\.\\d+)?)\\s*(kg|kilogram)', 1000),\n        (r'(\\d+(?:\\.\\d+)?)\\s*(lb|pound)', 453.592),\n    ]\n    \n    volume_value = 0.0\n    for pattern, multiplier in volume_patterns:\n        match = re.search(pattern, text_lower)\n        if match:\n            volume_value = float(match.group(1)) * multiplier\n            break\n    \n    features['volume_value'] = volume_value\n    features['log_volume'] = np.log1p(volume_value)\n    features['total_volume'] = volume_value * pack_qty\n    features['log_total_volume'] = np.log1p(volume_value * pack_qty)\n    \n    # ========== IPQ EXTRACTION ==========\n    ipq_match = re.search(r'Value:\\s*([\\d.]+)', text)\n    ipq_val = float(ipq_match.group(1)) if ipq_match else 1.0\n    features['ipq_value'] = ipq_val\n    features['log_ipq'] = np.log1p(ipq_val)\n    \n    unit_match = re.search(r'Unit:\\s*(\\w+)', text)\n    features['unit'] = unit_match.group(1) if unit_match else 'UNKNOWN'\n    \n    # ========== TEXT STATISTICS ==========\n    features['char_count'] = len(text)\n    features['word_count'] = len(text.split())\n    features['line_count'] = len(text.split('\\n'))\n    features['digit_count'] = sum(1 for c in text if c.isdigit())\n    features['upper_count'] = sum(1 for c in text if c.isupper())\n    features['comma_count'] = text.count(',')\n    features['newline_count'] = text.count('\\n')\n    features['avg_word_len'] = np.mean([len(w) for w in text.split()]) if text.split() else 0\n    \n    # ========== NUMERIC PATTERNS ==========\n    numbers = re.findall(r'\\d+\\.?\\d*', text)\n    if numbers:\n        nums = [float(n) for n in numbers]\n        features['num_count'] = len(nums)\n        features['num_max'] = max(nums)\n        features['num_min'] = min(nums)\n        features['num_mean'] = np.mean(nums)\n        features['num_sum'] = sum(nums)\n        features['num_std'] = np.std(nums) if len(nums) > 1 else 0\n        features['num_range'] = max(nums) - min(nums)\n        features['num_skew'] = skew(nums) if len(nums) > 2 else 0\n    else:\n        features.update({\n            'num_count': 0, 'num_max': 0, 'num_min': 0,\n            'num_mean': 0, 'num_sum': 0, 'num_std': 0,\n            'num_range': 0, 'num_skew': 0\n        })\n    \n    # ========== CATEGORY INDICATORS ==========\n    features['is_food'] = int(any(kw in text_lower for kw in \n        ['food', 'snack', 'drink', 'beverage', 'nutrition', 'candy', 'chocolate', 'meal']))\n    features['is_health'] = int(any(kw in text_lower for kw in \n        ['vitamin', 'supplement', 'health', 'wellness', 'medicine', 'protein']))\n    features['is_beauty'] = int(any(kw in text_lower for kw in \n        ['beauty', 'skincare', 'lotion', 'cream', 'cosmetic', 'shampoo', 'soap']))\n    features['is_household'] = int(any(kw in text_lower for kw in \n        ['cleaner', 'detergent', 'paper', 'tissue', 'towel', 'trash', 'laundry']))\n    features['is_baby'] = int(any(kw in text_lower for kw in \n        ['baby', 'infant', 'diaper', 'wipes', 'formula']))\n    features['is_organic'] = int(any(kw in text_lower for kw in \n        ['organic', 'natural', 'non-gmo', 'gluten-free']))\n    \n    # ========== PRICE INDICATORS ==========\n    features['has_value'] = int(any(kw in text_lower for kw in ['value', 'economy', 'saver']))\n    features['has_premium'] = int(any(kw in text_lower for kw in ['premium', 'deluxe', 'gourmet', 'luxury']))\n    features['has_bulk'] = int(any(kw in text_lower for kw in ['bulk', 'wholesale', 'family size']))\n    \n    # ========== INTERACTION FEATURES ==========\n    features['value_per_pack'] = ipq_val / max(pack_qty, 1)\n    features['total_quantity'] = ipq_val * pack_qty\n    features['log_total_qty'] = np.log1p(ipq_val * pack_qty)\n    features['words_per_line'] = features['word_count'] / max(features['line_count'], 1)\n    features['density'] = features['char_count'] / max(features['word_count'], 1)\n    features['digit_ratio'] = features['digit_count'] / max(features['char_count'], 1)\n    features['upper_ratio'] = features['upper_count'] / max(features['char_count'], 1)\n    \n    # Volume per pack\n    if volume_value > 0 and pack_qty > 0:\n        features['volume_per_pack'] = volume_value / pack_qty\n    else:\n        features['volume_per_pack'] = 0\n    \n    return features\n\n# ==================== LOAD DATA ====================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LOADING DATA\")\nprint(\"=\" * 80)\n\ntrain_df = pd.read_csv(TRAIN_CSV)\ntest_df = pd.read_csv(TEST_CSV)\n\nprint(f\"Train shape: {train_df.shape}\")\nprint(f\"Test shape: {test_df.shape}\")\n\n# ==================== OUTLIER REMOVAL ====================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"HANDLING OUTLIERS\")\nprint(\"=\" * 80)\n\n# More aggressive outlier removal\nQ1 = train_df['price'].quantile(0.01)\nQ3 = train_df['price'].quantile(0.99)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\noriginal_len = len(train_df)\ntrain_df = train_df[(train_df['price'] >= lower_bound) & \n                    (train_df['price'] <= upper_bound)].reset_index(drop=True)\n\nprint(f\"Outliers removed: {original_len} → {len(train_df)} ({original_len - len(train_df)} removed)\")\n\ntrain_df['log_price'] = np.log1p(train_df['price'])\n\n# CREATE PRICE BUCKETS for stratified sampling\ntrain_df['price_bucket'] = pd.qcut(train_df['price'], q=10, labels=False, duplicates='drop')\n\n# ==================== LOAD EMBEDDINGS ====================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LOADING EMBEDDINGS\")\nprint(\"=\" * 80)\n\nX_train_img = None\nX_test_img = None\nX_train_text_emb = None\nX_test_text_emb = None\n\n# Load image embeddings\ntry:\n    print(\"Loading image embeddings...\")\n    with open(TRAIN_IMG_EMB_PKL, 'rb') as f:\n        train_img_data = pickle.load(f)\n    with open(TEST_IMG_EMB_PKL, 'rb') as f:\n        test_img_data = pickle.load(f)\n    \n    X_train_img_raw = train_img_data['embeddings']\n    X_test_img = test_img_data['embeddings']\n    \n    if 'ids' in train_img_data:\n        emb_ids = train_img_data['ids']\n        train_sample_ids = train_df['sample_id'].values\n        mask = np.isin(emb_ids, train_sample_ids)\n        X_train_img = X_train_img_raw[mask]\n    else:\n        if len(X_train_img_raw) > len(train_df):\n            X_train_img = X_train_img_raw[:len(train_df)]\n        else:\n            X_train_img = X_train_img_raw\n    \n    if len(X_train_img) != len(train_df):\n        if len(X_train_img) < len(train_df):\n            mean_emb = np.mean(X_train_img, axis=0, keepdims=True)\n            padding = np.repeat(mean_emb, len(train_df) - len(X_train_img), axis=0)\n            X_train_img = np.vstack([X_train_img, padding])\n        else:\n            X_train_img = X_train_img[:len(train_df)]\n    \n    print(f\"✓ Image embeddings loaded: Train {X_train_img.shape}, Test {X_test_img.shape}\")\nexcept Exception as e:\n    print(f\"⚠️ Image embeddings not loaded: {e}\")\n\n# Load text embeddings\ntry:\n    print(\"Loading text embeddings...\")\n    with open(TEXT_EMB_PKL, 'rb') as f:\n        text_emb_data = pickle.load(f)\n    \n    X_train_text_emb_raw = text_emb_data['train_embeddings']\n    X_test_text_emb = text_emb_data['test_embeddings']\n    \n    if 'metadata' in text_emb_data and 'train_sample_ids' in text_emb_data['metadata']:\n        emb_sample_ids = text_emb_data['metadata']['train_sample_ids']\n        train_sample_ids = train_df['sample_id'].values\n        mask = np.isin(emb_sample_ids, train_sample_ids)\n        X_train_text_emb = X_train_text_emb_raw[mask]\n    else:\n        if len(X_train_text_emb_raw) > len(train_df):\n            X_train_text_emb = X_train_text_emb_raw[:len(train_df)]\n        else:\n            X_train_text_emb = X_train_text_emb_raw\n    \n    if len(X_train_text_emb) != len(train_df):\n        if len(X_train_text_emb) < len(train_df):\n            padding = np.zeros((len(train_df) - len(X_train_text_emb), X_train_text_emb.shape[1]))\n            X_train_text_emb = np.vstack([X_train_text_emb, padding])\n        else:\n            X_train_text_emb = X_train_text_emb[:len(train_df)]\n    \n    print(f\"✓ Text embeddings loaded: Train {X_train_text_emb.shape}, Test {X_test_text_emb.shape}\")\nexcept Exception as e:\n    print(f\"⚠️ Text embeddings not loaded: {e}\")\n\n# ==================== FEATURE ENGINEERING ====================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"COMPREHENSIVE FEATURE ENGINEERING\")\nprint(\"=\" * 80)\n\nprint(\"Extracting comprehensive features...\")\ntrain_df['clean_text'] = train_df['catalog_content'].fillna('').apply(clean_text)\ntest_df['clean_text'] = test_df['catalog_content'].fillna('').apply(clean_text)\n\ntrain_features = train_df['catalog_content'].fillna('').apply(\n    lambda x: pd.Series(extract_comprehensive_features(x)))\ntest_features = test_df['catalog_content'].fillna('').apply(\n    lambda x: pd.Series(extract_comprehensive_features(x)))\n\nprint(f\"✓ Extracted {len(train_features.columns)} engineered features\")\n\n# TF-IDF with optimized parameters\nprint(\"Creating TF-IDF features...\")\ntfidf = TfidfVectorizer(\n    max_features=20000,  # Increased\n    ngram_range=(1, 3),\n    min_df=2,\n    max_df=0.90,\n    sublinear_tf=True,\n    strip_accents='unicode'\n)\n\nX_tfidf_train = tfidf.fit_transform(train_df['clean_text'])\nX_tfidf_test = tfidf.transform(test_df['clean_text'])\n\n# SVD reduction\nprint(\"Applying SVD dimensionality reduction...\")\nsvd = TruncatedSVD(n_components=200, random_state=42)  # Increased\nX_svd_train = svd.fit_transform(X_tfidf_train)\nX_svd_test = svd.transform(X_tfidf_test)\n\nprint(f\"✓ TF-IDF → SVD: {X_tfidf_train.shape} → {X_svd_train.shape}\")\n\n# ==================== EMBEDDING CLUSTERING ====================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CREATING EMBEDDING-BASED CLUSTERS\")\nprint(\"=\" * 80)\n\n# Cluster embeddings to create categorical features\nif X_train_text_emb is not None:\n    print(\"Clustering text embeddings...\")\n    n_clusters = 50\n    kmeans_text = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    train_text_clusters = kmeans_text.fit_predict(X_train_text_emb)\n    test_text_clusters = kmeans_text.predict(X_test_text_emb)\n    \n    # One-hot encode clusters\n    train_text_cluster_onehot = np.eye(n_clusters)[train_text_clusters]\n    test_text_cluster_onehot = np.eye(n_clusters)[test_text_clusters]\n    print(f\"✓ Text embedding clusters: {n_clusters}\")\nelse:\n    train_text_cluster_onehot = None\n    test_text_cluster_onehot = None\n\nif X_train_img is not None:\n    print(\"Clustering image embeddings...\")\n    n_clusters = 30\n    kmeans_img = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    train_img_clusters = kmeans_img.fit_predict(X_train_img)\n    test_img_clusters = kmeans_img.predict(X_test_img)\n    \n    train_img_cluster_onehot = np.eye(n_clusters)[train_img_clusters]\n    test_img_cluster_onehot = np.eye(n_clusters)[test_img_clusters]\n    print(f\"✓ Image embedding clusters: {n_clusters}\")\nelse:\n    train_img_cluster_onehot = None\n    test_img_cluster_onehot = None\n\n# ==================== COMBINE ALL FEATURES ====================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"COMBINING FEATURES\")\nprint(\"=\" * 80)\n\n# Numeric features\nnumeric_cols = [col for col in train_features.columns if col != 'unit']\nX_numeric_train = train_features[numeric_cols].fillna(0).values\nX_numeric_test = test_features[numeric_cols].fillna(0).values\n\n# Scale numeric features with QuantileTransformer for better distribution\nscaler = QuantileTransformer(n_quantiles=1000, output_distribution='normal', random_state=42)\nX_numeric_train_scaled = scaler.fit_transform(X_numeric_train)\nX_numeric_test_scaled = scaler.transform(X_numeric_test)\n\n# Combine features\nfeature_list_train = [X_svd_train, X_numeric_train_scaled]\nfeature_list_test = [X_svd_test, X_numeric_test_scaled]\nfeature_names = [\"TF-IDF+SVD\", \"Numeric\"]\n\nif X_train_img is not None:\n    # More components from image embeddings\n    pca_img = PCA(n_components=100, random_state=42)  # Increased\n    X_img_reduced_train = pca_img.fit_transform(X_train_img)\n    X_img_reduced_test = pca_img.transform(X_test_img)\n    feature_list_train.append(X_img_reduced_train)\n    feature_list_test.append(X_img_reduced_test)\n    feature_names.append(\"Image(PCA)\")\n    \n    if train_img_cluster_onehot is not None:\n        feature_list_train.append(train_img_cluster_onehot)\n        feature_list_test.append(test_img_cluster_onehot)\n        feature_names.append(\"ImgClusters\")\n\nif X_train_text_emb is not None:\n    pca_text = PCA(n_components=100, random_state=42)  # Increased\n    X_text_reduced_train = pca_text.fit_transform(X_train_text_emb)\n    X_text_reduced_test = pca_text.transform(X_test_text_emb)\n    feature_list_train.append(X_text_reduced_train)\n    feature_list_test.append(X_text_reduced_test)\n    feature_names.append(\"TextEmb(PCA)\")\n    \n    if train_text_cluster_onehot is not None:\n        feature_list_train.append(train_text_cluster_onehot)\n        feature_list_test.append(test_text_cluster_onehot)\n        feature_names.append(\"TextClusters\")\n\nX_train_combined = np.hstack(feature_list_train)\nX_test_combined = np.hstack(feature_list_test)\n\nprint(f\"Feature groups: {' + '.join(feature_names)}\")\nprint(f\"Final shape: Train {X_train_combined.shape}, Test {X_test_combined.shape}\")\n\ny_train = train_df['log_price'].values\nprice_buckets = train_df['price_bucket'].values\n\n# ==================== ENSEMBLE TRAINING ====================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TRAINING ENHANCED ENSEMBLE\")\nprint(\"=\" * 80)\n\n# Use StratifiedKFold for better validation split\nskf = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)  # Increased folds\n\nlgb_oof = np.zeros(len(X_train_combined))\nxgb_oof = np.zeros(len(X_train_combined))\nridge_oof = np.zeros(len(X_train_combined))\n\nlgb_test_preds = []\nxgb_test_preds = []\nridge_test_preds = []\n\n# Enhanced LightGBM parameters\nlgb_params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting_type': 'gbdt',\n    'n_estimators': 5000,  # Increased\n    'learning_rate': 0.02,  # Decreased for more iterations\n    'max_depth': 10,  # Increased\n    'num_leaves': 255,  # Increased\n    'min_child_samples': 10,  # Decreased\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 0.05,\n    'reg_lambda': 0.05,\n    'random_state': 42,\n    'n_jobs': -1,\n    'verbose': -1,\n    'extra_trees': True  # Added for regularization\n}\n\n# Enhanced XGBoost parameters\nxgb_params = {\n    'objective': 'reg:squarederror',\n    'tree_method': 'hist',\n    'n_estimators': 4000,  # Increased\n    'learning_rate': 0.02,  # Decreased\n    'max_depth': 9,  # Increased\n    'min_child_weight': 1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 0.05,\n    'reg_lambda': 0.05,\n    'gamma': 0.1,  # Added\n    'random_state': 42\n}\n\n# Ridge regression parameters\nridge_params = {\n    'alpha': 10.0,\n    'random_state': 42\n}\n\nprint(\"\\nTraining 7-Fold Stratified Cross-Validation Ensemble...\")\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_train_combined, price_buckets)):\n    print(f\"\\nFold {fold + 1}/7\")\n    \n    X_tr, X_val = X_train_combined[train_idx], X_train_combined[val_idx]\n    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n    \n    # Train LightGBM\n    print(\"  Training LightGBM...\")\n    lgb_model = lgb.LGBMRegressor(**lgb_params)\n    lgb_model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_val, y_val)],\n        callbacks=[lgb.early_stopping(150, verbose=False)]\n    )\n    lgb_oof[val_idx] = lgb_model.predict(X_val)\n    lgb_test_preds.append(lgb_model.predict(X_test_combined))\n    \n    # Train XGBoost\n    print(\"  Training XGBoost...\")\n    xgb_model = xgb.XGBRegressor(**xgb_params)\n    xgb_model.fit(\n        X_tr, y_tr,\n        eval_set=[(X_val, y_val)],\n        verbose=False\n    )\n    xgb_oof[val_idx] = xgb_model.predict(X_val)\n    xgb_test_preds.append(xgb_model.predict(X_test_combined))\n    \n    # Train Ridge\n    print(\"  Training Ridge...\")\n    ridge_model = Ridge(**ridge_params)\n    ridge_model.fit(X_tr, y_tr)\n    ridge_oof[val_idx] = ridge_model.predict(X_val)\n    ridge_test_preds.append(ridge_model.predict(X_test_combined))\n    \n    # Fold metrics\n    lgb_fold_smape = smape(np.expm1(y_val), np.expm1(lgb_oof[val_idx]))\n    xgb_fold_smape = smape(np.expm1(y_val), np.expm1(xgb_oof[val_idx]))\n    ridge_fold_smape = smape(np.expm1(y_val), np.expm1(ridge_oof[val_idx]))\n    \n    print(f\"  LGB: {lgb_fold_smape:.4%}, XGB: {xgb_fold_smape:.4%}, Ridge: {ridge_fold_smape:.4%}\")\n\n# Average test predictions\nlgb_test_pred = np.mean(lgb_test_preds, axis=0)\nxgb_test_pred = np.mean(xgb_test_preds, axis=0)\nridge_test_pred = np.mean(ridge_test_preds, axis=0)\n\n# ==================== STACKING ====================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STACKING LAYER\")\nprint(\"=\" * 80)\n\n# Create stacking features\nX_stack_train = np.column_stack([lgb_oof, xgb_oof, ridge_oof])\nX_stack_test = np.column_stack([lgb_test_pred, xgb_test_pred, ridge_test_pred])\n\n# Train meta-learner\nprint(\"Training meta-learner (Ridge)...\")\nmeta_model = Ridge(alpha=1.0, random_state=42)\nmeta_model.fit(X_stack_train, y_train)\n\nfinal_oof = meta_model.predict(X_stack_train)\nfinal_test_pred = meta_model.predict(X_stack_test)\n\nstack_smape = smape(np.expm1(y_train), np.expm1(final_oof))\nprint(f\"✓ Stacked OOF SMAPE: {stack_smape:.4%}\")\n\n# Print meta-model weights\nprint(f\"Meta-model weights: LGB={meta_model.coef_[0]:.3f}, XGB={meta_model.coef_[1]:.3f}, Ridge={meta_model.coef_[2]:.3f}\")\n\n# ==================== GENERATE SUBMISSION ====================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"GENERATING SUBMISSION\")\nprint(\"=\" * 80)\n\nfinal_test_pred = np.expm1(final_test_pred)\nfinal_test_pred = np.maximum(final_test_pred, 0.01)\n\nsubmission = pd.DataFrame({\n    'sample_id': test_df['sample_id'],\n    'price': final_test_pred\n})\n\noutput_file = os.path.join(OUTPUT_PATH, 'submission_v2.csv')\nsubmission.to_csv(output_file, index=False)\n\nprint(f\"\\n✓ Submission saved to '{output_file}'\")\nprint(f\"  Rows: {len(submission)}\")\nprint(f\"  Price range: ${submission['price'].min():.2f} - ${submission['price'].max():.2f}\")\nprint(f\"  Mean: ${submission['price'].mean():.2f}, Median: ${submission['price'].median():.2f}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"FINAL OOF SMAPE: {stack_smape:.4%}\")\nprint(\"=\" * 80)\n\nprint(\"\\nFirst 20 predictions:\")\nprint(submission.head(20))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:22:46.251317Z","iopub.execute_input":"2025-10-13T13:22:46.252120Z","iopub.status.idle":"2025-10-13T16:27:47.805037Z","shell.execute_reply.started":"2025-10-13T13:22:46.252086Z","shell.execute_reply":"2025-10-13T16:27:47.804386Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nENHANCED PRICE PREDICTION V2 - TARGET <45% SMAPE\n================================================================================\n\n================================================================================\nLOADING DATA\n================================================================================\nTrain shape: (75000, 4)\nTest shape: (75000, 3)\n\n================================================================================\nHANDLING OUTLIERS\n================================================================================\nOutliers removed: 75000 → 74943 (57 removed)\n\n================================================================================\nLOADING EMBEDDINGS\n================================================================================\nLoading image embeddings...\n✓ Image embeddings loaded: Train (74943, 512), Test (75000, 512)\nLoading text embeddings...\n✓ Text embeddings loaded: Train (74943, 384), Test (75000, 384)\n\n================================================================================\nCOMPREHENSIVE FEATURE ENGINEERING\n================================================================================\nExtracting comprehensive features...\n✓ Extracted 42 engineered features\nCreating TF-IDF features...\nApplying SVD dimensionality reduction...\n✓ TF-IDF → SVD: (74943, 20000) → (74943, 200)\n\n================================================================================\nCREATING EMBEDDING-BASED CLUSTERS\n================================================================================\nClustering text embeddings...\n✓ Text embedding clusters: 50\nClustering image embeddings...\n✓ Image embedding clusters: 30\n\n================================================================================\nCOMBINING FEATURES\n================================================================================\nFeature groups: TF-IDF+SVD + Numeric + Image(PCA) + ImgClusters + TextEmb(PCA) + TextClusters\nFinal shape: Train (74943, 521), Test (75000, 521)\n\n================================================================================\nTRAINING ENHANCED ENSEMBLE\n================================================================================\n\nTraining 7-Fold Stratified Cross-Validation Ensemble...\n\nFold 1/7\n  Training LightGBM...\n  Training XGBoost...\n  Training Ridge...\n  LGB: 48.3397%, XGB: 49.1120%, Ridge: 56.5427%\n\nFold 2/7\n  Training LightGBM...\n  Training XGBoost...\n  Training Ridge...\n  LGB: 48.0749%, XGB: 48.8441%, Ridge: 56.3328%\n\nFold 3/7\n  Training LightGBM...\n  Training XGBoost...\n  Training Ridge...\n  LGB: 47.7451%, XGB: 48.6641%, Ridge: 56.4241%\n\nFold 4/7\n  Training LightGBM...\n  Training XGBoost...\n  Training Ridge...\n  LGB: 48.2590%, XGB: 48.8015%, Ridge: 56.8016%\n\nFold 5/7\n  Training LightGBM...\n  Training XGBoost...\n  Training Ridge...\n  LGB: 48.3549%, XGB: 49.2076%, Ridge: 56.5237%\n\nFold 6/7\n  Training LightGBM...\n  Training XGBoost...\n  Training Ridge...\n  LGB: 47.6725%, XGB: 48.5982%, Ridge: 56.2217%\n\nFold 7/7\n  Training LightGBM...\n  Training XGBoost...\n  Training Ridge...\n  LGB: 47.8994%, XGB: 48.8121%, Ridge: 56.1381%\n\n================================================================================\nSTACKING LAYER\n================================================================================\nTraining meta-learner (Ridge)...\n✓ Stacked OOF SMAPE: 47.5357%\nMeta-model weights: LGB=0.887, XGB=0.288, Ridge=-0.132\n\n================================================================================\nGENERATING SUBMISSION\n================================================================================\n\n✓ Submission saved to '/kaggle/working/submission_v2.csv'\n  Rows: 75000\n  Price range: $0.45 - $333.93\n  Mean: $19.42, Median: $13.81\n\n================================================================================\nFINAL OOF SMAPE: 47.5357%\n================================================================================\n\nFirst 20 predictions:\n    sample_id      price\n0      100179  14.694717\n1      245611  15.392646\n2      146263  23.500748\n3       95658   9.630080\n4       36806  28.457649\n5      148239   4.561589\n6       92659   7.043545\n7        3780  12.125854\n8      196940  14.000000\n9       20472   7.447822\n10     121721  27.424277\n11     127336   6.528369\n12      20801  46.605535\n13      30103   7.727282\n14      68691  19.967786\n15     230703  14.636038\n16       3342  28.470618\n17     115635  16.435528\n18     221565   9.488143\n19     265927  15.064551\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}